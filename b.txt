<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8" />
    <!-- Load MathJax (example: version 3) for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6" ></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    <title>Evaluation Dimensions</title>
</head>
<body>

<h1>Evaluation Dimensions</h1>

<!-- 1. FLUENCY -->
<h2>1. Fluency</h2>
<p>
Fluency measures whether the generated text is grammatically correct, naturally phrased, and follows the typical conventions of the target language.
</p>

<!-- Example: Perplexity (PPL) formula -->
<p>
Using a language model, we often compute Perplexity (PPL) to approximate fluency. A lower perplexity implies higher fluency:
</p>
<p>
$$
\text{PPL}(T) = \exp\Biggl(-\frac{1}{N} \sum_{i=1}^{N} \log_2 \, p(w_i \mid w_{1}, \ldots, w_{i-1})\Biggr)
$$
</p>
<p>
where \( T \) is the text of length \( N \). \( w_i \) is the \( i \)-th token, and \( p(w_i \mid w_{1}, \ldots, w_{i-1}) \) is the probability of that token given the preceding context.
</p>

<hr />

<!-- 2. CONSISTENCY -->
<h2>2. Consistency</h2>
<p>
Consistency (also referred to as "accuracy") assesses whether the generated text aligns with the known facts, context, or source information. Inconsistency can manifest as factual errors or contradictions.
</p>

<!-- Example formula: Contradiction-based Consistency -->
<p>
One approach is to count contradictions against a reference set of facts or a knowledge base. A simple metric could be:
</p>
<p>
$$
\text{Consistency}(T, C)
= 1 \;-\; \frac{\text{ContradictionCount}(T, C)}{\text{AllFactCount}(T)}
$$
</p>
<p>
where \( T \) is the generated text, \( C \) is the set of known facts/context,
<em>ContradictionCount</em> is the number of factual conflicts, and
<em>AllFactCount</em> is the number of verifiable facts mentioned in \( T \). The closer this value is to 1, the better the consistency.
</p>

<hr />

<!-- 3. COHERENCE -->
<h2>3. Coherence</h2>
<p>
Coherence evaluates how well the text is structured logically and how smoothly ideas flow from one sentence to the next.
</p>

<!-- Example formula: Average pairwise sentence similarity -->
<p>
A simple automatic measure uses sentence embeddings and calculates the average similarity between consecutive sentences:
</p>
<p>
$$
\text{Coherence}(T) =
\frac{1}{m-1} \sum_{i=1}^{m-1}
\text{Sim}\bigl(\mathbf{v}(S_i), \mathbf{v}(S_{i+1})\bigr)
$$
</p>
<p>
where \( S_1, S_2, \ldots, S_m \) are the sentences in \( T \),
\( \mathbf{v}(S_i) \) is the embedding of the \( i \)-th sentence (e.g., using BERT or similar),
and \( \text{Sim} \) is a similarity function such as cosine similarity. A higher value indicates more coherent text.
</p>

<hr />

<!-- 4. RELEVANCE -->
<h2>4. Relevance</h2>
<p>
Relevance indicates how well the generated text addresses a given query, topic, or reference text.
</p>

<!-- Example formula: Similarity-based Relevance -->
<p>
When you have a reference text \( R \) (or a query), you can measure relevance by comparing vector embeddings:
</p>
<p>
$$
\text{Relevance}(T, R) = \text{Sim}\bigl(\mathbf{v}(T), \mathbf{v}(R)\bigr)
$$
</p>
<p>
where \( \text{Sim}(\cdot,\cdot) \) could be the cosine similarity. Higher values imply better alignment with the topic or query.
</p>

</body>
</html>
